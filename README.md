# LLM Evaluation Pipeline

A production-ready evaluation pipeline for assessing AI conversation quality using LLM-as-a-Judge methodology with hallucination detection, relevance scoring, and cost optimization.

![Frontend Screenshot](assets/front-end.png)

---

## ðŸ“‹ Table of Contents
- [Local Setup Instructions](#local-setup-instructions)
- [Testing the Pipeline](#testing-the-pipeline)
- [Architecture](#architecture)
- [Design Decisions](#design-decisions)
- [Scale & Performance Optimization](#scale--performance-optimization)
- [Important Notes](#important-notes)

---

## ðŸš€ Local Setup Instructions

### Prerequisites
- Docker & Docker Compose installed
- 16GB+ RAM recommended
- For GPU setup: NVIDIA GPU with CUDA support, WSL2 (Windows), NVIDIA Container Toolkit

### Option 1: CPU Setup (Prebuilt Images)

**Step 1:** Clone the repository
```bash
git clone <repository-url>
cd LLM-eval-pipeline
```

**Step 2:** Start services using prebuilt images
```bash
docker-compose -f docker-compose.prebuilt.yml up -d
```

**Step 3:** Pull the Ollama model (first run only, ~4-5GB)
```bash
docker exec judge-llm ollama pull qwen2.5:7b
```
Wait until download completes. You can monitor progress with:
```bash
docker logs -f judge-llm
```

**Step 4:** Verify all services are running
```bash
docker ps
```
You should see 4 containers: `judge-llm`, `vector-encoder`, `evaluation-service`, `frontend`

### Option 2: GPU Setup (Prebuilt Images with NVIDIA GPU)

**Prerequisites:**
- NVIDIA GPU with CUDA support
- WSL2 installed (Windows users)
- NVIDIA Container Toolkit installed in WSL2

**Install NVIDIA Container Toolkit (WSL2):**
```bash
# Add NVIDIA package repository
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# Install nvidia-docker2
sudo apt-get update
sudo apt-get install -y nvidia-docker2

# Restart Docker
sudo systemctl restart docker
```

**Start services with GPU support:**
```bash
docker-compose -f docker-compose.prebuilt.gpu.yml up -d
```

**Pull the Ollama model:**
```bash
docker exec judge-llm ollama pull qwen2.5:7b
```

**Verify GPU is being used:**
```bash
docker exec judge-llm nvidia-smi
```

### Option 3: Build from Source

```bash
docker-compose up --build -d
```

---

## ðŸ§ª Testing the Pipeline

**Note:** All test payloads are located in the `data/` folder.

### Test Payload 1: Hallucination Detection (Subsidized Rooms)
```bash
curl -X POST http://localhost:8000/evaluate \
  -H "Content-Type: application/json" \
  -d @data/test_payload.json
```

**Expected Result:**
- Overall Score: ~82.61
- Hallucinations Detected: 1 (Turn 14: "subsidized rooms" claim)
- Turns Evaluated: 15

### Test Payload 2: Clean Conversation
```bash
curl -X POST http://localhost:8000/evaluate \
  -H "Content-Type: application/json" \
  -d @data/test_payload_2.json
```

**Expected Result:**
- Overall Score: ~91.08
- Hallucinations Detected: 0
- All responses grounded in context

### Test Payload 3: Question vs Claim Test
```bash
curl -X POST http://localhost:8000/evaluate \
  -H "Content-Type: application/json" \
  -d @data/test_payload_3.json
```

**Expected Result:**
- Overall Score: ~91.08
- Hallucinations Detected: 0 (Turn 15 contains a question, not a factual claim)

### View Results in Frontend
Open browser: `http://localhost:3000`

### Check Individual Service Health
```bash
# Evaluation Service
curl http://localhost:8000/health

# Vector Encoder
curl http://localhost:8001/health

# Judge LLM
curl http://localhost:11434/api/tags
```

---

## ðŸ—ï¸ Architecture

### System Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Client Request                          â”‚
â”‚                    (Conversation + Vectors)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Evaluation Service (Port 8000)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Receive conversation turns + context vectors          â”‚  â”‚
â”‚  â”‚  2. For each turn:                                        â”‚  â”‚
â”‚  â”‚     - Extract AI response + user query                    â”‚  â”‚
â”‚  â”‚     - Send to Vector Encoder for MaxSim selection         â”‚  â”‚
â”‚  â”‚     - Send selected vector + response to Judge LLM        â”‚  â”‚
â”‚  â”‚  3. Calculate scores (R, C, H, L, K)                      â”‚  â”‚
â”‚  â”‚  4. Return evaluation results                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                  â”‚
               â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Encoder (Port 8001)  â”‚   â”‚  Judge LLM (Port 11434)     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ MaxSim Vector Selectionâ”‚  â”‚   â”‚  â”‚ Ollama (qwen2.5:7b)   â”‚  â”‚
â”‚  â”‚ - Encode AI response   â”‚  â”‚   â”‚  â”‚ - Hallucination check â”‚  â”‚
â”‚  â”‚ - Encode all vectors   â”‚  â”‚   â”‚  â”‚ - Relevance scoring   â”‚  â”‚
â”‚  â”‚ - Calculate cosine sim â”‚  â”‚   â”‚  â”‚ - Completeness check  â”‚  â”‚
â”‚  â”‚ - Return most relevant â”‚  â”‚   â”‚  â”‚ - JSON output         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Frontend (Port 3000)     â”‚
                  â”‚  - Real-time results      â”‚
                  â”‚  - WebSocket updates      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Container Architecture

**1. Evaluation Service (FastAPI)**
- Orchestrates evaluation workflow
- Calculates weighted scores: H(40%), C(30%), R(20%), L(5%), K(5%)
- Handles turn-by-turn evaluation
- Exposes REST API on port 8000

**2. Vector Encoder (FastAPI + sentence-transformers)**
- Uses `all-MiniLM-L6-v2` model for embeddings
- Implements MaxSim algorithm for vector selection
- Selects single most relevant context vector per turn
- Exposes REST API on port 8001

**3. Judge LLM (Ollama + qwen2.5:7b)**
- Runs Qwen 2.5 7B model for evaluation
- Detects hallucinations by comparing response to context
- Scores relevance and completeness
- Temperature=0.1 for consistency
- Exposes Ollama API on port 11434

**4. Frontend (FastAPI + WebSockets)**
- Real-time evaluation results display
- WebSocket for live updates
- Simple HTML/JS interface
- Exposes web UI on port 3000

### Data Flow

1. **Request Ingestion**: Client sends conversation JSON with turns and vectors
2. **Vector Selection**: For each turn, MaxSim selects most relevant context vector
3. **LLM Evaluation**: Judge LLM evaluates response against selected vector
4. **Score Calculation**: Weighted scoring across 5 dimensions
5. **Response**: JSON with per-turn scores and overall evaluation

---

## ðŸ¤” Design Decisions

### Why This Architecture?

**1. Microservices Separation**
- **Reason**: Enables independent scaling of compute-intensive components (LLM, embeddings)
- **Alternative Rejected**: Monolithic service would bottleneck on LLM inference
- **Benefit**: Can scale vector-encoder horizontally without scaling expensive LLM instances

**2. MaxSim Vector Selection (Not Top-K)**
- **Reason**: Reduces LLM context size from 3 vectors to 1, cutting costs by 66%
- **Alternative Rejected**: Passing all vectors increases token usage and latency
- **Implementation**: Calculate cosine similarity between response embedding and all vector embeddings, select max
- **Benefit**: Maintains accuracy while minimizing tokens sent to LLM

**3. LLM-as-a-Judge (Qwen 2.5 7B)**
- **Reason**: Balances accuracy with inference speed; better than rule-based, cheaper than GPT-4
- **Alternative Rejected**: Entailment models (BERT-based) miss nuanced hallucinations
- **Benefit**: Detects subtle factual errors that keyword matching misses

**4. Temperature=0.1 (Not 0.0)**
- **Reason**: Prevents model from getting stuck in repetitive patterns while maintaining consistency
- **Alternative Rejected**: Temperature=0.0 caused identical outputs even for different inputs
- **Benefit**: Balances determinism with model flexibility

**5. Weighted Scoring Strategy**
- **Reason**: Hallucination (40%) weighted highest as it's most critical for trust
- **Formula**: `Overall = 0.4H + 0.3C + 0.2R + 0.05L + 0.05K`
- **Alternative Rejected**: Equal weighting doesn't reflect business priorities
- **Benefit**: Aligns scores with real-world impact (hallucinations > latency)

**6. Docker Compose (Not Kubernetes)**
- **Reason**: Simplifies local development and testing
- **Alternative**: K8s adds complexity without benefit for single-node deployments
- **Benefit**: Easy setup, reproducible environments, GPU support via docker-compose

---

## âš¡ Scale & Performance Optimization

### Handling Millions of Daily Conversations

**1. Vector Selection Optimization (66% Cost Reduction)**
- **Problem**: Passing 3 vectors per turn = 3x token costs
- **Solution**: MaxSim selects only 1 most relevant vector
- **Impact**: Reduces LLM input tokens by 66%, cutting inference costs proportionally
- **Scale**: At 1M conversations/day with 10 turns each, saves ~$2000/day in LLM costs

**2. Async Processing Pipeline**
- **Implementation**: FastAPI async/await for non-blocking I/O
- **Benefit**: Single instance handles 1000+ concurrent requests
- **Scale**: Reduces server count by 10x compared to synchronous processing

**3. Embedding Model Optimization**
- **Choice**: `all-MiniLM-L6-v2` (80MB, 120ms inference)
- **Alternative Rejected**: `all-mpnet-base-v2` (420MB, 350ms inference)
- **Impact**: 3x faster embeddings, 5x smaller memory footprint
- **Scale**: Enables 10 vector-encoder replicas on single GPU vs 2 replicas

**4. LLM Inference Optimization**
- **Batch Size**: 512 tokens per batch
- **Context Window**: 4096 tokens (sufficient for 1 vector + response)
- **Quantization Ready**: Qwen 2.5 supports 4-bit quantization (4x memory reduction)
- **Scale**: Single GPU handles 50 req/sec with quantization vs 12 req/sec without

**5. Caching Strategy (Future Enhancement)**
- **Approach**: Cache vector embeddings (vectors rarely change)
- **Impact**: Eliminates 90% of embedding computations
- **Implementation**: Redis cache with 24hr TTL
- **Scale**: Reduces vector-encoder load by 10x

**6. Horizontal Scaling Architecture**
```
Load Balancer
    â”œâ”€â”€ Evaluation Service (10 replicas) - Stateless, CPU-bound
    â”œâ”€â”€ Vector Encoder (5 replicas) - GPU-accelerated
    â””â”€â”€ Judge LLM (3 replicas) - GPU-intensive, most expensive
```
- **Evaluation Service**: Cheap to scale (CPU only)
- **Vector Encoder**: Moderate cost (small GPU)
- **Judge LLM**: Scale last (expensive, but MaxSim reduces load)

**7. Latency Optimization**
- **Current**: ~2-3s per turn (LLM inference dominates)
- **Target**: <1s per turn at scale
- **Techniques**:
  - Speculative decoding (2x speedup)
  - Continuous batching (3x throughput)
  - Model distillation (Qwen 2.5 1.5B, 5x faster)
- **Scale**: Handles 10M conversations/day with 30 LLM replicas

**8. Kubernetes Deployment (Future Production Scale)**
- **Approach**: Deploy on K8s with HorizontalPodAutoscaler for each microservice
- **Scaling Strategy**:
  - Evaluation Service: Auto-scale 5-50 replicas (CPU-based)
  - Vector Encoder: Auto-scale 3-20 replicas (GPU-based)
  - Judge LLM: Auto-scale 2-10 replicas (GPU-intensive, most expensive)
- **Benefits**:
  - Independent scaling per service based on load
  - Rolling updates with zero downtime
  - Resource optimization with node affinity (GPU nodes for LLM/encoder)
  - Cost efficiency: Scale down during low traffic
- **Implementation**: Helm charts with resource limits, health checks, and auto-scaling policies

**9. Cost Breakdown (1M conversations/day, 10 turns each)**
```
Without MaxSim:
- LLM tokens: 10M turns Ã— 3 vectors Ã— 500 tokens = 15B tokens/day
- Cost: ~$3000/day (self-hosted GPU amortized)

With MaxSim:
- LLM tokens: 10M turns Ã— 1 vector Ã— 500 tokens = 5B tokens/day
- Cost: ~$1000/day (66% reduction)
```

---

## ðŸ“ Important Notes

### **âš ï¸ Input JSON Modifications**
**We have merged the two separate input JSON files into a single unified format and corrected syntax errors in the original JSON structure.** The original files contained:
- Invalid JSON syntax (missing commas, incorrect nesting)
- Separate conversation and vector files requiring manual merging
- Inconsistent timestamp formats

Our unified format (`data/test_payload.json`, `data/test_payload_2.json`, `data/test_payload_3.json`) provides:
- Valid JSON structure
- Single file containing both conversation turns and context vectors
- Consistent schema for easy testing

### Hallucination Detection Philosophy
- **Questions are NOT hallucinations**: "Do you have questions about legal contracts?" is a question, not a factual claim
- **Only factual claims are evaluated**: Statements like "We offer subsidized rooms" are checked against context
- **Severity weighting**: Medical/safety hallucinations penalized more than logistics errors

### Model Selection
- **Qwen 2.5 7B** chosen for balance of accuracy and speed
- Alternatives tested: Llama 3 8B (slower), Mistral 7B (less accurate on hallucinations)
- Temperature tuning: 0.1 provides best consistency without sacrificing quality

### Known Limitations
- First run downloads 4-5GB model (one-time)
- CPU inference is slow (~10s per turn); GPU recommended for production
- Evaluation is sequential per conversation (parallelizable across conversations)

---

## ðŸ“Š Scoring Methodology

### Score Components
- **Relevance (R)**: Does response address the query? (20% weight)
- **Completeness (C)**: Is response thorough? (30% weight)
- **Hallucination (H)**: Any false information? (40% weight)
- **Latency (L)**: Response time vs SLA (5% weight)
- **Cost (K)**: Token usage vs budget (5% weight)

### Overall Score Formula
```
Overall = (R Ã— 0.2) + (C Ã— 0.3) + (H Ã— 0.4) + (L Ã— 0.05) + (K Ã— 0.05)
```

### Hallucination Severity
- **Critical (1.0)**: Medical, health, safety claims
- **High (0.7)**: Financial, legal, contractual claims
- **Medium (0.5)**: General factual claims
- **Low (0.3)**: Logistics, metadata (room numbers, times)

---

## ðŸ› ï¸ Troubleshooting

### Services won't start
```bash
# Check logs
docker-compose -f docker-compose.prebuilt.yml logs

# Restart services
docker-compose -f docker-compose.prebuilt.yml restart
```

### Ollama model not loading
```bash
# Check if model is downloaded
docker exec judge-llm ollama list

# Pull model if missing
docker exec judge-llm ollama pull qwen2.5:7b
```

### GPU not detected
```bash
# Verify NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# Check docker daemon.json
cat /etc/docker/daemon.json
# Should contain: {"default-runtime": "nvidia"}
```

---

## ðŸ“¦ Repository Structure
```
LLM-eval-pipeline/
â”œâ”€â”€ data/                          # Test payloads
â”‚   â”œâ”€â”€ test_payload.json         # Hallucination test case
â”‚   â”œâ”€â”€ test_payload_2.json       # Clean conversation
â”‚   â””â”€â”€ test_payload_3.json       # Question vs claim test
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ evaluation-service/       # Main orchestrator
â”‚   â”œâ”€â”€ vector-encoder/           # MaxSim vector selection
â”‚   â””â”€â”€ judge-llm/                # Ollama LLM container
â”œâ”€â”€ frontend/                      # Web UI
â”œâ”€â”€ docker-compose.yml            # Build from source
â”œâ”€â”€ docker-compose.prebuilt.yml   # CPU prebuilt images
â””â”€â”€ docker-compose.prebuilt.gpu.yml # GPU prebuilt images
```

---

## ðŸš€ Quick Start (TL;DR)
```bash
# Start services
docker-compose -f docker-compose.prebuilt.yml up -d

# Pull Ollama model (first run)
docker exec judge-llm ollama pull qwen2.5:7b

# Test evaluation (payloads in data/ folder)
curl -X POST http://localhost:8000/evaluate \
  -H "Content-Type: application/json" \
  -d @data/test_payload.json

# View in browser
open http://localhost:3000
```

---

**Built with â¤ï¸ for scalable, production-ready LLM evaluation**

---

## ðŸ“§ Contact

**Prakhar Dixit**
- Email: prakhardixit0312@gmail.com
- Phone: +91 9979949981
